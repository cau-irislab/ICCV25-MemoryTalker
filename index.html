<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MemoryTalker: Personalized Speech-Driven 3D Facial Animation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization</h1>
            <h2 class="title is-3">ICCV 2025</h2>
                  <div class="is-size-5 publication-authors">
                    <!-- Paper authors -->
                    <span class="author-block">
                      <a href="#" target="_blank">Hyung Kyu Kim</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="#" target="_blank">Sangmin Lee</a><sup>2,*</sup>,</span>
                    <span class="author-block">
                      <a href="#" target="_blank">Hak Gu Kim</a><sup>3,*</sup>
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="eql-cntrb"><br><sup>1</sup>Department of Imaging Science and Arts, Chung-Ang University, South Korea</span>
                    <span class="eql-cntrb"><br><sup>2</sup>Department of Computer Science and Engineering, Korea University, South Korea</span>
                    <span class="eql-cntrb"><br><sup>3</sup>Department of Metaverse Convergence, Chung-Ang University, South Korea</span>
                    <span class="eql-cntrb"><br><sup>*</sup>Corresponding Author</span>
                  </div>

                  

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/kimhyungkyu-1208/MemoryTalker" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        MemoryTalker: Realistic 3D facial animation from audio alone — no priors, just input voice. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/iccv25.png" alt="The intuition of MemoryTalker for personalized speech-driven 3D facial animation.">
        </figure>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: The 1-stage is storing and retrieving general motion (i.e., Memorizing), and the 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Limitation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitation</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/fig_limitation.png" alt="The intuition of MemoryTalker for personalized speech-driven 3D facial animation.">
        </figure>
        <div class="content has-text-justified">
          <p>
            Previous studies on personalized 3D facial animation have practical limitations. Methods based on 
            (a) one-hot encoding require speaker identity classes during inference, making it impossible for them to handle new, unseen speakers. This severely limits their practical application.
            (b) a sequence of 3D facial meshes to control for different speaking styles. While these methods can capture the styles of unseen speakers, they are impractical for real-world applications because they require this additional 3D mesh data as input during inference.
            (c) Our proposed MemoryTalker overcomes these limitations. It is designed to reflect a speaker's style using only the audio input. MemoryTalker does not require any prior information—such as speaker ID classes or reference facial meshes—at inference time, making it a more practical approach for real-world applications
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Limitation -->

<!-- Paper Proposed Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/fig_method.png" alt="The intuition of MemoryTalker for personalized speech-driven 3D facial animation.">
        </figure>
        <div class="content has-text-justified">
          <p>
            We propose MemoryTalker, a novel framework for personalized speech-driven 3D facial animation that can reflect an individual's speaking style using only audio input. Our method uses a two-stage training strategy: 1) Memorizing general facial motions and 2) Animating personalized motions using a stylized memory.  
          </p>
          <p>
            Stage 1: Storing and Recalling General Facial Motion (Memorizing)
            In the first stage, our goal is to create a Motion Memory that stores general facial motion features. To achieve this, we use the text representation extracted from an audio signal by an Automatic Speech Recognition (ASR) model as a query to access the memory. This allows the model to map the facial motions of various speakers for a single phoneme to a consistent representation, ensuring that it learns general and speaker-neutral facial movements (e.g., the common lip shape for the word "who"). The recalled general motion feature is then passed to a motion decoder to synthesize a base facial animation.
          </p>
          <p>
            Stage 2: Generating Personalized Facial Motion (Animating)
            In the second stage, we synthesize a personalized facial animation that reflects the speaker's unique style. To do this, we first encode a speaking style feature from the input audio using a style encoder trained with a triplet loss, which helps distinguish between different speakers' styles. This style feature is then used to refine and update the motion memory from Stage 1, creating a Stylized Motion Memory. This stylized memory now contains personalized motion information. Finally, the personalized motion feature is recalled and used by the motion decoder to generate the final 3D facial animation that accurately reflects both the speech content and the individual's speaking style.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Proposed Method -->

<!-- Paper Qualitative Result -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Result</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/fig_qualitative_comparison.png" alt="The intuition of MemoryTalker for personalized speech-driven 3D facial animation.">
        </figure>
        <div class="content has-text-justified">
          <p>
            This figure presents a qualitative comparison of our MemoryTalker against state-of-the-art speech-driven 3D facial animation methods on the VOCASET and BIWI datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Qualitative Result -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Study: Facial Movement Analysis</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/ablation1.png" alt="Ablation study comparing facial movements with and without 2-stage training for the word Stab.">
        </figure>
        <div class="content has-text-justified">
          <p>
            [cite_start]This analysis demonstrates the effectiveness of our proposed <strong>2-stage training strategy</strong> by visualizing the facial dynamics for the word "Stab"[cite: 6]. [cite_start]The model trained only with the 1st stage (top row, 'w/o 2-stage') produces animations with limited and less expressive lip motion[cite: 1]. [cite_start]This is quantitatively supported by the Mean of 3D Optical Flow, where its value ($0.897 \times 10^{-4}$) is significantly lower than the ground truth reference ($1.367 \times 10^{-4}$)[cite: 7, 9]. [cite_start]In contrast, after incorporating the 2nd stage for personalization (middle row, 'w/ 2-stage'), the model generates much more dynamic and accurate movements that closely match the reference[cite: 2, 3]. [cite_start]The optical flow value ($1.278 \times 10^{-4}$) becomes nearly identical to the ground truth[cite: 8, 9]. This visually confirms that our two-stage approach is crucial for capturing the subtle dynamics of realistic facial expressions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Study: Speaker Style Disentanglement (t-SNE)</h2>

        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/ablation2.png" alt="t-SNE visualization of speaker style features with and without 2-stage training.">
        </figure>
        <div class="content has-text-justified">
          <p>
            This t-SNE visualization illustrates how our two-stage training effectively disentangles unique speaker styles. [cite_start]In the left plot (a), showing the feature distribution without the 2nd stage, the motion features from different speakers (represented by different colors) are heavily overlapped and poorly separated[cite: 16]. This indicates the model is only learning generic, non-personalized motions. However, the right plot (b) shows the result after our full two-stage training. [cite_start]The features for each speaker form distinct, well-defined clusters[cite: 10, 11, 12, 13, 14, 15, 16]. This proves that our model successfully learns to capture and separate personalized speaking styles from audio alone, which is essential for generating realistic, identity-specific animations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/fig_method.png" alt="Method" style="display: block; max-width: 70%; margin: auto;"/>
        <h2 class="subtitle has-text-centered">
          Overview of Our Proposed Method
        </h2>
      </div>
      <div class="item">
        <img src="static/images/fig_qualitative_comparison.png" alt="Qualitative comparison" style="display: block; max-width: 70%; margin: auto;"/>
        <h2 class="subtitle has-text-centered">
          Qualitative Comparison
        </h2>
      </div>
      <div class="item">
        <img src="static/images/fig_limitation.png" alt="Limitation" style="display: block; max-width: 70%; margin: auto;"/>
        <h2 class="subtitle has-text-centered">
         Limitations of Existing Methods
       </h2>
     </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/iccv25_poster-sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{MemoryTalker,
        author = {Kim, Hyung Kyu and Lee, Sangmin and Kim, Hak Gu}, 
        title = {MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization},
        journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 
        month = {October},
        year = {2025},
        pages = {n-n+7}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
